# Generated by CodiumAI
import time
import unittest
from unittest.mock import patch
from langchain.schema import (HumanMessage, AIMessage, SystemMessage)

from autogpt.config import Config
from autogpt.chat import generate_context


class TestChat(unittest.TestCase):
    # Tests the behavior of the generate_context function when all input parameters are empty.
    @patch("time.strftime")
    def test_generate_context_empty_inputs(self, mock_strftime):
        # Mock the time.strftime function to return a fixed value
        mock_strftime.return_value = "Sat Apr 15 00:00:00 2023"
        # Arrange
        CFG = Config()
        CFG.openai_api_key = "test" # Needed to instantiate ChatOpenAI
        prompt = ""
        relevant_memory = ""
        full_message_history = []
        model = "gpt-3.5-turbo-0301"

        # Act
        result = generate_context(prompt, relevant_memory, full_message_history, model)

        # Assert
        expected_result = (
            -1,
            47,
            3,
            [
                SystemMessage(content=""),
                SystemMessage(content=f"The current time and date is {time.strftime('%c')}"),
                SystemMessage(content=f"This reminds you of these events from your past:\n\n\n"),
            ],
        )
        self.assertEqual(result, expected_result)

    # Tests that the function successfully generates a current_context given valid inputs.
    def test_generate_context_valid_inputs(self):
        # Given
        prompt = "What is your favorite color?"
        relevant_memory = "You once painted your room blue."
        full_message_history = [
            HumanMessage(content="Hi there!"),
            AIMessage(content="Hello! How can I assist you today?"),
            HumanMessage(content="Can you tell me a joke?"),
            AIMessage(content="Why did the tomate turn red? Because it saw the salad dressing!"),
            HumanMessage(content="Haha, that's funny."),
        ]
        model = "gpt-3.5-turbo-0301"

        # When
        result = generate_context(prompt, relevant_memory, full_message_history, model)

        # Then
        self.assertIsInstance(result[0], int)
        self.assertIsInstance(result[1], int)
        self.assertIsInstance(result[2], int)
        self.assertIsInstance(result[3], list)
        self.assertGreaterEqual(result[0], 0)
        self.assertGreaterEqual(result[1], 0)
        self.assertGreaterEqual(result[2], 0)
        self.assertGreaterEqual(
            len(result[3]), 3
        )  # current_context should have at least 3 messages
        self.assertLessEqual(
            result[1], 2048
        )  # token limit for GPT-3.5-turbo-0301 is 2048 tokens
